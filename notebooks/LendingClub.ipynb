{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lending Club Prediction Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following was inspired by a very good yhat blog post that created a prediction model for Lending Club loans using R:\n",
    "http://blog.yhathq.com/posts/machine-learning-for-predicting-bad-loans.html\n",
    "\n",
    "The point of this exercise was to translate the yhat post into Python as much as possible and use sci-kit learn to evaluate several Machine Learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "import model_eval\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "Lending Club helpfully publishes anonymized loan data in csv format, which we'll use for this analysis.\n",
    "You have to have a Lending Club log in to access the data, which is available here:\n",
    "https://www.lendingclub.com/info/download-data.action\n",
    "\n",
    "I've copied the files used in this analysis to a public Dropbox folder here:\n",
    "https://dl.dropboxusercontent.com/u/12406727/Data/\n",
    "\n",
    "To start, we'll load the files into a single pandas data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# file_base_url = \"https://dl.dropboxusercontent.com/u/12406727/Data/{0}\"\n",
    "file_base_url = '~/Dropbox/Public/Data/{0}'\n",
    "\n",
    "files = ['LoanStats3a_securev1.csv', \n",
    "         'LoanStats3b_securev1.csv', \n",
    "         'LoanStats3c_securev1.csv', \n",
    "         'LoanStats3d_securev1.csv']\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for f in files:\n",
    "    d = pd.read_csv(file_base_url.format(f), low_memory=False, \n",
    "                          index_col='id', header=1, parse_dates=['issue_d'])\n",
    "    print len(d)\n",
    "    df = df.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Record:', len(df)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep & Cleanup\n",
    "We'll need to do a bit of data cleanup and we'll follow the yhat post pretty closely here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del df['desc']\n",
    "del df['mths_since_last_record']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns with more than 80% NA values\n",
    "This also drops any totals columns that might have snuck in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = df[df.member_id > 0].dropna(axis=1,thresh=df.member_id.count()*.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(data)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag bad loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_indicators = ['Late (16-30 days)', 'Late (31-120 days)', 'Default', 'Charged Off']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set is_bad indicator to true to loans in status with 'bad' indicators\n",
    "data.loc[:,'is_bad'] = False\n",
    "data.loc[data['loan_status'].isin(bad_indicators), 'is_bad'] = True\n",
    "data.loc[data['loan_status'] ==\"\", 'is_bad'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how many bad loans overall?\n",
    "print 'Total loans:\\t', data.member_id.count()\n",
    "print 'Bad loans:\\t', data[data['is_bad']==True].member_id.count()\n",
    "print 'Bad loan %:\\t', data[data['is_bad']==True].member_id.count()*1./data.member_id.count()*1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Type Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_date(x):\n",
    "    try:\n",
    "        return pd.datetime.strptime(x, '%b-%Y')\n",
    "    except:\n",
    "        print x\n",
    "        raise\n",
    "        \n",
    "# sometimes this necessary, sometimes not...probably depends on version of pandas installed\n",
    "# dateparse = lambda x: convert_date(x)\n",
    "# data.loc[:,'issue_d'] = data.issue_d.map(dateparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.loc[:,'issue_d'] = pd.to_datetime(data.issue_d)\n",
    "data.loc[:,'year_issued'] = data.issue_d.dt.year\n",
    "data.loc[:,'month_issued'] = data.issue_d.dt.month\n",
    "data.loc[:,'earliest_cr_line'] = pd.to_datetime(data.earliest_cr_line)\n",
    "data.loc[:,'revol_util'] = data['revol_util'].str.replace(\"[%]\", \"\").astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.home_ownership = data.home_ownership.astype('category')\n",
    "data.loc[:,'is_rent'] = False\n",
    "data.loc[data['home_ownership'].isin(['RENT']), 'is_rent'] = True\n",
    "data.loc[:,'fico_range'] = data.fico_range_high.astype('category')\n",
    "data.loc[:,'fico_ordered'] = data.fico_range_high.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.groupby('fico_ordered').member_id.count();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.crosstab(data.year_issued, data.loan_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data.groupby('year_issued').is_bad.sum()/data.groupby('year_issued').is_bad.count()).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get only mature loans to make sure they had enough to be paid off "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mature_loans = data[data['year_issued'] <= 2012]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Total loans:\\t', len(mature_loans)\n",
    "print 'Bad loans:\\t', len(mature_loans[mature_loans['is_bad']==True])\n",
    "print 'Bad loan %:\\t', len(mature_loans[mature_loans['is_bad']==True])*1./len(mature_loans)*1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(mature_loans.groupby('year_issued').is_bad.sum()/mature_loans.groupby('year_issued').is_bad.count()).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mature_loans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_cols = ['loan_amnt', 'annual_inc', 'fico_range_low', 'fico_range_high', \n",
    "                'inq_last_6mths', 'open_acc', 'pub_rec', 'revol_bal', 'total_acc', \n",
    "                'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', \n",
    "                'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', \n",
    "                'recoveries', 'collection_recovery_fee', 'last_pymnt_amnt', \n",
    "                'last_fico_range_high', 'last_fico_range_low',\n",
    "                'is_rent']\n",
    "\n",
    "label = ['is_bad']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = mature_loans[feature_cols].fillna(0)\n",
    "y = mature_loans[label].fillna(0)#.values\n",
    "print 'X:', X.shape\n",
    "print 'y:', y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale numeric columns\n",
    "We'll scale all numeric columns by adjusting them by their means and standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stdsc = StandardScaler()\n",
    "\n",
    "X_scaled = pd.DataFrame(stdsc.fit_transform(X), columns=feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Modeling\n",
    "We'll use sci-kit learn for all models, to create a model designed to predict whether a loan will be 'bad'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try some basic logistic regression first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "log = LogisticRegression(C=1e3, penalty='l2') # params learned from GridSearchCV\n",
    "log, log_data_split, log_y_score = model_eval.run_prediction(X_scaled, y, log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = LogisticRegression(C=1e3, penalty='l2') # params learned from GridSearchCV\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=.25)\n",
    "    \n",
    "#fit on training data\n",
    "fit = model.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print \"Accuracy of model:\\t\", accuracy\n",
    "print 'MSE\\t', metrics.mean_squared_error(y_test, y_pred)\n",
    "print 'RMSE\\t', np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "score_train = model.score(X_train, y_train)\n",
    "score_test = model.score(X_test, y_test)\n",
    "\n",
    "print '\\n'\n",
    "print '-----------------------------------------'\n",
    "print 'Scores:'\n",
    "print '-----------------------------------------'\n",
    "print 'Train\\t', score_train\n",
    "print 'Test\\t', score_test\n",
    "print '-----------------------------------------\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(model, X_scaled, y['is_bad'].values, cv=10, scoring='accuracy', n_jobs=1, verbose=1)\n",
    "score_cross_val_mean = scores.mean()\n",
    "print 'Mean Cross-Val Score:', score_cross_val_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_y_test = log_data_split['y_test']\n",
    "log_y_pred = log_data_split['y_pred']\n",
    "\n",
    "cm, cm_norm = model_eval.confusion_matrix(log_y_test, log_y_pred)\n",
    "\n",
    "# print cm_norm\n",
    "model_eval.plot_confusion_matrix(cm_norm)\n",
    "# test_score = model_eval.model_metrics(log, X, y, log_data_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print '\\n'\n",
    "print 'Coefficients for each X:\\n'\n",
    "coeff = pd.DataFrame(zip(feature_cols, log.coef_[0]), columns=['Feature', 'Weight'])\n",
    "coeff.sort_values(by='Weight', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = log.predict_proba(log_data_split['X_test'])\n",
    "model_eval.plot_histogram(p[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_eval.plot_roc(log_data_split['y_test'], log_y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "C_range = 10.0 ** np.arange(-2, 9)\n",
    "\n",
    "param_grid = [{\n",
    "              'C': C_range, \n",
    "                'penalty': ['l1', 'l2']\n",
    "             }]\n",
    "\n",
    "log = LogisticRegression()\n",
    "print X_scaled.shape\n",
    "# print y[:,0].values.shape\n",
    "print param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " y['is_bad'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(log, param_grid, scoring='accuracy', n_jobs = 1)\n",
    "grid.fit(X_scaled.values,  y['is_bad'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'best params', grid.best_params_\n",
    "print 'best params', grid.best_estimator_\n",
    "print 'best params', grid.best_score_\n",
    "\n",
    "grid_mean_scores = [result.mean_validation_score for result in grid.grid_scores_]\n",
    "print grid_mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn, knn_data_split, knn_y_score = run_prediction(X, y, knn)\n",
    "\n",
    "knn_y_test = knn_data_split['y_test']\n",
    "knn_y_pred = knn_data_split['y_pred']\n",
    "\n",
    "confusion_matrix(knn_y_test, knn_y_pred)\n",
    "\n",
    "model_metrics(knn, X, y, knn_data_split)\n",
    "# scores = cross_validation(knn, X, y, n_jobs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = knn.predict_proba(knn_data_split['X_test'])\n",
    "plot_histogram(p[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using GridSearch to test n_neighbors parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# n_range = range(1,11)\n",
    "# param_grid = dict(\n",
    "#               n_neighbors=n_range\n",
    "#              )\n",
    "# knn = KNeighborsClassifier()\n",
    "\n",
    "# grid = GridSearchCV(knn, param_grid, scoring='accuracy', n_jobs = 1)\n",
    "# grid.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_roc(knn_data_split['y_test'], knn_y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grid_mean_scores = [result.mean_validation_score for result in grid.grid_scores_]\n",
    "# print grid_mean_scores\n",
    "# print 'best params', grid.best_params_\n",
    "# print 'best params', grid.best_estimator_\n",
    "# print 'best params', grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.plot(n_range, score grid.grid_scores_)\n",
    "# plt.xlabel('Value of N for n_neighbors')\n",
    "# plt.ylabel('Cross-Validated Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=5)\n",
    "print rf\n",
    "\n",
    "rf, rf_data_split, rf_y_score = run_prediction(X, y, rf)\n",
    "\n",
    "rf_y_test = rf_data_split['y_test']\n",
    "rf_y_pred = rf_data_split['y_pred']\n",
    "\n",
    "confusion_matrix(rf_y_test, rf_y_pred)\n",
    "\n",
    "model_metrics(log, X, y, rf_data_split)\n",
    "\n",
    "scores = cross_validation(rf, X, y, n_jobs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 0.977352599188\n",
    "p = rf.predict_proba(rf_data_split['X_test'])\n",
    "plot_histogram(p[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot_roc(rf_data_split['y_test'], rf_y_score)\n",
    "print pd.DataFrame(zip(cols, rf.feature_importances_)).sort(1, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.groupby(\"is_bad\")[\"recoveries\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=3000, max_features=1.0, learning_rate=0.01, \n",
    "                                    max_depth=4, min_samples_leaf=5)\n",
    "\n",
    "gb, data_split, y_score = model_eval.run_prediction(X_scaled, y, gb)\n",
    "\n",
    "y_test = data_split['y_test']\n",
    "y_pred = data_split['y_pred']\n",
    "\n",
    "model_eval.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "model_eval.model_metrics(log, X, y, data_split)\n",
    "# scores = cross_validation(log, X, y, n_jobs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print pd.DataFrame(zip(cols, gb.feature_importances_)).sort(1, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = gb.predict_proba(data_split['X_test'])\n",
    "plot_histogram(p[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_roc(data_split['y_test'], y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Selecting Hyperparameters With Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# est_range = [100, 150, 200]\n",
    "# depth_range = range(1,6)\n",
    "# n_scores = []\n",
    "param_grid = dict(\n",
    "              learning_rate=[0.1, 0.05, 0.02, 0.01],\n",
    "              max_depth=[4, 6],\n",
    "              min_samples_leaf=[3, 5, 9, 17],\n",
    "              max_features=[1.0, 0.3, 0.1]\n",
    "             )\n",
    "gb = GradientBoostingClassifier(n_estimators=3000)\n",
    "\n",
    "grid = GridSearchCV(gb, param_grid, scoring='accuracy', n_jobs = 4)\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'best params', grid.best_params_\n",
    "print 'best params', grid.best_estimator_\n",
    "print 'best params', grid.best_score_\n",
    "\n",
    "grid_mean_scores = [result.mean_validation_score for result in grid.grid_scores_]\n",
    "print grid_mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# r = pd.DataFrame(grid.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# res = zip([x['n_estimators'] for x in r[0]], [x['max_depth'] for x in r[0]], [x for x in r[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dr = pd.DataFrame(res)\n",
    "# dr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # plot the results\n",
    "# est_score = dr.groupby(dr[0]).mean()\n",
    "# print est_score\n",
    "\n",
    "# plt.plot(est_range, est_score[2])\n",
    "# plt.xlabel('Value of N for n_estimators')\n",
    "# plt.ylabel('Cross-Validated Accuracy')\n",
    "# plt.ylim(dr[2].min(), dr[2].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the results\n",
    "md_score = dr.groupby(dr[1]).mean()\n",
    "print md_score\n",
    "plt.plot(depth_range, md_score[2])\n",
    "plt.xlabel('Value of N for max_depth')\n",
    "plt.ylabel('Cross-Validated Accuracy')\n",
    "plt.ylim(dr[2].min(), dr[2].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(md_score[2]-md_score[2].mean())/(md_score[2].xmax()-est_score[2].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
